{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "#imports\n",
    "import requests\n",
    "import pandas as pd\n",
    "import os\n",
    "import datetime as dt\n",
    "from google.cloud import bigquery\n",
    "from google.cloud import storage\n",
    "from shapely.geometry  import shape, Point\n",
    "from geopy.geocoders import Nominatim\n",
    "import numpy as np\n",
    "import meteostat as ms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "CREDS = \"C:/Users/tkkim/gcp_keys/capstone-team51-366963bafc54.json\"\n",
    "storage_client = storage.Client.from_service_account_json(json_credentials_path=CREDS,project='capstone-team51')\n",
    "bq_client = bigquery.Client.from_service_account_json(json_credentials_path=CREDS,project='capstone-team51')\n",
    "#client = bigquery.Client(project='capstone-team51')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "bucket = storage_client.get_bucket('capstone-team51-data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "timestamp = dt.datetime.now()\n",
    "timestamp = timestamp.strftime(\"%Y%m%d\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "loglist = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getmaxdate(bq_client):\n",
    "    QUERY = (\n",
    "        'SELECT MAX(date) FROM `capstone-team51.chicago_data_prod.consolidated_table` '\n",
    "        )\n",
    "    query_job = bq_client.query(QUERY)  # API request\n",
    "    df = query_job.to_dataframe()  # Waits for query to finish\n",
    "    maxdate = df.iloc[0,0].to_pydatetime().date().strftime(\"%Y-%m-%d\")\n",
    "    return maxdate\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "#primary data pull function\n",
    "\n",
    "def get_data(name, db, maxrows, parameters, loglist, bucket, filepath):\n",
    "    # some key info\n",
    "    db_id = db\n",
    "    limit = maxrows\n",
    "    i = 0\n",
    "    offset_counter = 0\n",
    "    pickle = True\n",
    "    data_name = name\n",
    "    # dict for logging\n",
    "    log_dict = {}\n",
    "\n",
    "    # base url for the request\n",
    "    baseurl = (\"https://data.cityofchicago.org/resource/\"\n",
    "            f\"{db_id}.json?\")\n",
    "\n",
    "    while pickle:\n",
    "        #set our params\n",
    "        # note that you have to URL econde \n",
    "        params = parameters + (\n",
    "            f\"&$limit={limit}\"\n",
    "            f\"&$offset={offset_counter}\"\n",
    "            #f\"&$$app_token={token}\"\n",
    "        )\n",
    "        \n",
    "        # make the request \n",
    "        data_req = requests.get(baseurl+params)\n",
    "\n",
    "        # convert to json\n",
    "        data_json = data_req.json()\n",
    "\n",
    "        # make it a df\n",
    "        df = pd.json_normalize(data_json)\n",
    "\n",
    "        # write df\n",
    "        \n",
    "        \n",
    "        # log\n",
    "        minilog = {}\n",
    "        minilog['name'] = name\n",
    "        minilog['records'] = df.shape[0]\n",
    "        minilog['chunk'] = i\n",
    "        minilog['status'] = data_req.status_code\n",
    "        minilog['offset'] = offset_counter\n",
    "        minilog['date'] = timestamp\n",
    "        \n",
    "        \n",
    "        if df.shape[0] == 0:\n",
    "            break\n",
    "        else:\n",
    "            bucket.blob(f'{filepath}/{timestamp}_{data_name}_chunk_{i}.csv').upload_from_string(df.to_csv(index=False), 'text/csv')\n",
    "            #df.to_json(f'{desti_folder}/{timestamp}_{data_name}_chunk_{i}.json',orient='records', lines=True)\n",
    "            loglist.append(minilog)\n",
    "\n",
    "        # increment the chunk count\n",
    "        print(i)\n",
    "        i += 1\n",
    "\n",
    "        # increment the offset\n",
    "        offset_counter += limit\n",
    "\n",
    "        # check if we need to end the loop\n",
    "        if df.shape[0] == 0:\n",
    "            pickle = False\n",
    "        elif data_req.status_code == 200:\n",
    "            pickle = True\n",
    "        else: \n",
    "            pickle = False\n",
    "            print(data_req.status_code)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "datetime.date(2024, 2, 4)"
      ]
     },
     "execution_count": 156,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dt.datetime.now().date()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_weather_data(name, start_date, minilog, bucket, filepath):\n",
    "    ## weather\n",
    "    start = dt.datetime.strptime(start_date, '%Y-%m-%d').date()\n",
    "    end = dt.datetime.now().date()\n",
    "    # Create Point for chicago\n",
    "    chicago = ms.Point(41.85, -87.65)\n",
    "\n",
    "    dailyholder = ms.Daily(chicago, start, end)\n",
    "    data_daily = dailyholder.fetch()\n",
    "    data_daily[['tempmax_f', 'tempmin_f', 'tempavg_f']] = data_daily[['tmax', 'tmin', 'tavg']].apply(lambda x: (x * 9/5) + 32)\n",
    "    bucket.blob(f'{filepath}/{timestamp}_weather_daily_data.csv').upload_from_string(data_daily.to_csv(), 'text/csv')\n",
    "    #data_daily.to_csv(f'./data/{timestamp}_weather_daily_data.csv')\n",
    "\n",
    "    minilog = {}\n",
    "    minilog['name'] = name\n",
    "    minilog['records'] = data_daily.shape[0]\n",
    "    minilog['chunk'] = 0\n",
    "    minilog['status'] = 200\n",
    "    minilog['offset'] = 0\n",
    "    minilog['date'] = timestamp\n",
    "    #log_df = pd.DataFrame.from_dict(minilog,orient='index')\n",
    "    loglist.append(minilog)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "datetime.date(2023, 12, 31)"
      ]
     },
     "execution_count": 159,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dt.datetime.strptime(start_date, '%Y-%m-%d').date()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "datetime.date(2024, 2, 4)"
      ]
     },
     "execution_count": 160,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dt.datetime.now().date()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "unsupported operand type(s) for -: 'datetime.datetime' and 'datetime.date'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[158], line 6\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;66;03m# Create Point for chicago\u001b[39;00m\n\u001b[0;32m      4\u001b[0m chicago \u001b[38;5;241m=\u001b[39m ms\u001b[38;5;241m.\u001b[39mPoint(\u001b[38;5;241m41.85\u001b[39m, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m87.65\u001b[39m)\n\u001b[1;32m----> 6\u001b[0m dailyholder \u001b[38;5;241m=\u001b[39m \u001b[43mms\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mDaily\u001b[49m\u001b[43m(\u001b[49m\u001b[43mchicago\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstart\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mend\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      7\u001b[0m data_daily \u001b[38;5;241m=\u001b[39m dailyholder\u001b[38;5;241m.\u001b[39mfetch()\n\u001b[0;32m      8\u001b[0m data_daily[[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtempmax_f\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtempmin_f\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtempavg_f\u001b[39m\u001b[38;5;124m'\u001b[39m]] \u001b[38;5;241m=\u001b[39m data_daily[[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtmax\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtmin\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtavg\u001b[39m\u001b[38;5;124m'\u001b[39m]]\u001b[38;5;241m.\u001b[39mapply(\u001b[38;5;28;01mlambda\u001b[39;00m x: (x \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m9\u001b[39m\u001b[38;5;241m/\u001b[39m\u001b[38;5;241m5\u001b[39m) \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m32\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\tkkim\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\meteostat\\interface\\daily.py:98\u001b[0m, in \u001b[0;36mDaily.__init__\u001b[1;34m(self, loc, start, end, model, flags)\u001b[0m\n\u001b[0;32m     88\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\n\u001b[0;32m     89\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m     90\u001b[0m     loc: Union[pd\u001b[38;5;241m.\u001b[39mDataFrame, Point, \u001b[38;5;28mlist\u001b[39m, \u001b[38;5;28mstr\u001b[39m],  \u001b[38;5;66;03m# Station(s) or geo point\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     96\u001b[0m \n\u001b[0;32m     97\u001b[0m     \u001b[38;5;66;03m# Initialize time series\u001b[39;00m\n\u001b[1;32m---> 98\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_init_time_series\u001b[49m\u001b[43m(\u001b[49m\u001b[43mloc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstart\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mend\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mflags\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\tkkim\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\meteostat\\interface\\timeseries.py:156\u001b[0m, in \u001b[0;36mTimeSeries._init_time_series\u001b[1;34m(self, loc, start, end, model, flags)\u001b[0m\n\u001b[0;32m    154\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_stations \u001b[38;5;241m=\u001b[39m loc\u001b[38;5;241m.\u001b[39mindex\n\u001b[0;32m    155\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(loc, Point):\n\u001b[1;32m--> 156\u001b[0m     stations \u001b[38;5;241m=\u001b[39m \u001b[43mloc\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_stations\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mdaily\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstart\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mend\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    157\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_stations \u001b[38;5;241m=\u001b[39m stations\u001b[38;5;241m.\u001b[39mindex\n\u001b[0;32m    158\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\tkkim\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\meteostat\\interface\\point.py:92\u001b[0m, in \u001b[0;36mPoint.get_stations\u001b[1;34m(self, freq, start, end, model)\u001b[0m\n\u001b[0;32m     90\u001b[0m \u001b[38;5;66;03m# Apply inventory filter\u001b[39;00m\n\u001b[0;32m     91\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m freq \u001b[38;5;129;01mand\u001b[39;00m start \u001b[38;5;129;01mand\u001b[39;00m end:\n\u001b[1;32m---> 92\u001b[0m     age \u001b[38;5;241m=\u001b[39m (\u001b[43mdatetime\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnow\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mend\u001b[49m)\u001b[38;5;241m.\u001b[39mdays\n\u001b[0;32m     93\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m model \u001b[38;5;241m==\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m age \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m180\u001b[39m:\n\u001b[0;32m     94\u001b[0m         stations \u001b[38;5;241m=\u001b[39m stations\u001b[38;5;241m.\u001b[39minventory(freq, (start, end))\n",
      "\u001b[1;31mTypeError\u001b[0m: unsupported operand type(s) for -: 'datetime.datetime' and 'datetime.date'"
     ]
    }
   ],
   "source": [
    "start = dt.datetime.strptime(start_date, '%Y-%m-%d').date()\n",
    "end = dt.datetime.now().date()\n",
    "# Create Point for chicago\n",
    "chicago = ms.Point(41.85, -87.65)\n",
    "\n",
    "dailyholder = ms.Daily(chicago, start, end)\n",
    "data_daily = dailyholder.fetch()\n",
    "data_daily[['tempmax_f', 'tempmin_f', 'tempavg_f']] = data_daily[['tmax', 'tmin', 'tavg']].apply(lambda x: (x * 9/5) + 32)\n",
    "bucket.blob(f'{filepath}/{timestamp}_weather_daily_data.csv').upload_from_string(data_daily.to_csv(), 'text/csv')\n",
    "#data_daily.to_csv(f'./data/{timestamp}_weather_daily_data.csv')\n",
    "\n",
    "minilog = {}\n",
    "minilog['name'] = name\n",
    "minilog['records'] = data_daily.shape[0]\n",
    "minilog['chunk'] = 0\n",
    "minilog['status'] = 200\n",
    "minilog['offset'] = 0\n",
    "minilog['date'] = timestamp\n",
    "#log_df = pd.DataFrame.from_dict(minilog,orient='index')\n",
    "loglist.append(minilog)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Params and pulls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_date = getmaxdate(bq_client)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    }
   ],
   "source": [
    "# crime\n",
    "crime_params = (\n",
    "            f\"$where=DATE%20%3E%3D%20%27{start_date}%27\"\n",
    "            f\"&$order=DATE\"\n",
    "            #f\"&$$app_token={token}\"\n",
    "            )\n",
    "\n",
    "get_data('crime', 'ijzp-q8t2', 500000, crime_params, loglist, bucket, 'raw_crimes')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    }
   ],
   "source": [
    "# dph env complaints\n",
    "env_params = (\n",
    "            f\"$where=complaint_date%20%3E%3D%20%27{start_date}%27\"\n",
    "            f\"&$order=complaint_date\"\n",
    "            #f\"&$$app_token={token}\"\n",
    "            )\n",
    "\n",
    "get_data('dph_env', 'fypr-ksnz', 500000, env_params, loglist, bucket, 'raw_environmental')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    }
   ],
   "source": [
    "# 311 complaints\n",
    "three11_params = (\n",
    "            f\"$where=(created_date%20%3E%3D%20%27{start_date}%27)%20AND%20sr_short_code%20not%20in%20(%27311IOC%27)\"\n",
    "            f\"&$order=created_date\"\n",
    "            )\n",
    "\n",
    "get_data('311', 'v6vf-nfxy', 500000, three11_params, loglist, bucket, 'raw_311')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "# weather data\n",
    "get_weather_data('weather', start_date, loglist, bucket, 'raw_weather')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_df = pd.DataFrame(loglist)\n",
    "bucket.blob(f'logs/{timestamp}_pull_log.csv').upload_from_string(log_df.to_csv(index=False), 'text/csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
